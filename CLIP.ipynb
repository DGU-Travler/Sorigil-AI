{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP 모델 임포트 및 버전 설정, GPU에 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:25:29.901240Z",
     "start_time": "2024-09-25T06:25:20.261402Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COCO 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:25:38.485296Z",
     "start_time": "2024-09-25T06:25:37.499564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.58s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# COCO 데이터셋 경로 설정\n",
    "train_dataset = CocoCaptions(root=r'E:\\CLIP_COCO\\train2017',\n",
    "                             annFile=r'E:\\CLIP_COCO\\annotations\\captions_train2017.json',\n",
    "                             transform=transform)\n",
    "\n",
    "val_dataset = CocoCaptions(root=r'E:\\CLIP_COCO\\val2017',\n",
    "                           annFile=r'E:\\CLIP_COCO\\annotations\\captions_val2017.json',\n",
    "                           transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로드된 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:25:40.335537Z",
     "start_time": "2024-09-25T06:25:40.273149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([3, 480, 640])\n",
      "Caption: ['Closeup of bins of food that include broccoli and bread.', 'A meal is presented in brightly colored plastic trays.', 'there are containers filled with different kinds of foods', 'Colorful dishes holding meat, vegetables, fruit, and bread.', 'A bunch of trays that have different food.']\n"
     ]
    }
   ],
   "source": [
    "img, caption = train_dataset[0]\n",
    "print(\"Image size:\", img.size())\n",
    "print(\"Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# CLIP 모델 및 전처리 도구 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 컨텍스트 길이 설정 (CLIP의 기본값은 77)\n",
    "context_length = 77\n",
    "\n",
    "# Custom collate_fn\n",
    "def custom_collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    processed_images = []\n",
    "    processed_captions = []\n",
    "\n",
    "    for image in images:\n",
    "        # 이미지 데이터 체크 및 변환\n",
    "        if isinstance(image, np.ndarray):\n",
    "            # NumPy 배열을 PIL 이미지로 변환\n",
    "            image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            # 텐서를 PIL 이미지로 변환\n",
    "            image = Image.fromarray(image.numpy().astype('uint8'), 'RGB')\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported image type\")\n",
    "\n",
    "        # 이미지 전처리\n",
    "        processed_image = preprocess(image).unsqueeze(0)\n",
    "        processed_images.append(processed_image)\n",
    "\n",
    "    # 모든 이미지가 유효한지 확인\n",
    "    if len(processed_images) == 0:\n",
    "        raise RuntimeError(\"No valid images found in batch.\")\n",
    "\n",
    "    # 캡션 처리 및 길이 조정\n",
    "    for caption in captions:\n",
    "        if not isinstance(caption, str):\n",
    "            caption = str(caption)  # 비문자열 캡션을 문자열로 변환\n",
    "        # 캡션을 토큰화하고, 컨텍스트 길이 초과 시 자르기\n",
    "        tokens = clip.tokenize([caption], truncate=True)[0][:context_length].tolist()\n",
    "        processed_captions.append(tokens)\n",
    "\n",
    "    images = torch.cat(processed_images, dim=0)  # 이미지 텐서 결합\n",
    "    texts = torch.tensor(processed_captions).to(device)  # 캡션 텐서로 변환 및 GPU로 전송\n",
    "\n",
    "    return images, texts\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-25T06:25:43.544139Z",
     "start_time": "2024-09-25T06:25:41.808480Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# 모델 및 전처리 도구 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 최적화 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for images, texts in train_loader:\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "\n",
    "        # 모델을 통해 이미지와 텍스트 피처 추출\n",
    "        image_features = model.encode_image(images)\n",
    "        text_features = model.encode_text(texts)\n",
    "\n",
    "        # 유사도 계산\n",
    "        logits = (image_features @ text_features.T) / 0.07  # 온도 파라미터\n",
    "        labels = torch.arange(images.size(0), dtype=torch.long, device=device)\n",
    "\n",
    "        # 대조적 손실 계산\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'trained_model.pkl')\n",
    "print(\"모델 학습이 완료되었으며 trained_model.pkl 파일로 저장되었습니다.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
