{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "영문 원본 LLAVA - 허깅페이스에서 가져와서 pip 받을게 거의 없음"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8afe51053557ac0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:53:48.643204Z",
     "start_time": "2024-11-11T16:53:36.665420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Sorigil-AI\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: araffe cat with a piece of bread on its head\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# 이미지 불러오기\n",
    "image = Image.open('./testimage/test.png')\n",
    "\n",
    "# 이미지를 pixel_values로 변환 (전처리)\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# 캡션 생성\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "OCR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67db3a73975d3367"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated OCR text: 2.44\n",
      "al LTE) <\n",
      "103\n",
      "D2 파일이 공지로 등록되없습니다\n",
      "Vv 할 일 7H\n",
      "예약 메시지\n",
      "린아이에거 마시델로 1개틀 주고 15\n",
      "안 먹지 안고 참으면 2개틀 주기로\n",
      "올 때, 먹지\n",
      "참아서 2개틀 받은\n",
      "아이들이 커서 그렇지 않앗면 아이들 보\n",
      "더 훌륭하게 되없다는 심리학 실험인   다시델로\n",
      "야기\n",
      "있다. 미국에서 이\n",
      "험에 참가한 호아침 데 포사다는 같은 이름의\n",
      "자기계발서도 벗다. 하지만 이 -\n",
      "대해서 논란이\n",
      "이틀데면\n",
      "2013년 로체스터\n",
      "대달표데 \n",
      "돌리 팔데리\n",
      "와 리처드\n",
      "어측린은 구월_Coqnition-인 \"Rational 508\n",
      "Olooog\"o\n",
      "제목으로 발표한\n",
      "문에서 이 연\n",
      "결과에\n",
      "표하여 \"첫\n",
      "눈문예 액물굴\n",
      "빨리 덕은 아이들 중 일부는 참을\n",
      "부 즉햇단 것 이\n",
      "'나중에 들아오면 하나들 터 주컷다\n",
      "말을 의심\n",
      "'따문이라고 한다. 그들은\n",
      "안정 한 환경에서 자란 아이들은\n",
      "먹는 것이 남는\n",
      "이라는 생각올 갖게 된다\"눈 말 올 남기다, \"안정적인\n",
      "자란 아 이들\n",
      "일주록 약속이 뒷켜질 젓이라고\n",
      "대하여 좀 더\n",
      "오래\n",
      "오전 8.13\n",
      "오전 9.51\n",
      "안고\n",
      "라는\n",
      "성이\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "# EasyOCR 리더 초기화 (한글과 영어 지원)\n",
    "reader = easyocr.Reader(['ko', 'en'])\n",
    "\n",
    "# OCR 수행\n",
    "result = reader.readtext('/testimage/test6.png')\n",
    "text = '\\n'.join([res[1] for res in result])\n",
    "\n",
    "print(\"Generated OCR text:\", text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:54:01.851516Z",
     "start_time": "2024-11-11T16:53:57.954268Z"
    }
   },
   "id": "3101657425831ee8",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "ocr, 캡션 합치기"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cf27e5749b536e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import numpy as np\n",
    "\n",
    "# BLIP 모델과 프로세서 로드\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# EasyOCR 리더 초기화 (한글과 영어 지원)\n",
    "ocr_reader = easyocr.Reader(['ko', 'en'])\n",
    "\n",
    "# 이미지 불러오기\n",
    "image = Image.open('./testimage/test6.png').convert('RGB')\n",
    "\n",
    "# 1. EasyOCR을 사용하여 이미지 내 텍스트 추출 (PIL 이미지를 numpy 배열로 변환)\n",
    "ocr_result = ocr_reader.readtext(np.array(image))\n",
    "ocr_text = ' '.join([res[1] for res in ocr_result])\n",
    "\n",
    "# 2. BLIP를 사용하여 이미지 캡션 생성\n",
    "inputs = blip_processor(image, return_tensors=\"pt\")\n",
    "outputs = blip_model.generate(**inputs)\n",
    "caption = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 3. OCR 결과와 BLIP 캡션 결합\n",
    "combined_caption = f\"OCR Text: {ocr_text}\\nGenerated Caption: {caption}\"\n",
    "\n",
    "print(combined_caption)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c50761b069a5a4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "한글 llava 시도"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a125eec710befa90"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"KBNIT/KoLLaVA-1.5v-kolon-v1.6\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"KBNIT/KoLLaVA-1.5v-kolon-v1.6\")\n",
    "\n",
    "# 이미지 불러오기\n",
    "image = Image.open('test4.png')\n",
    "\n",
    "# 이미지를 pixel_values로 변환 (전처리)\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# 캡션 생성\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated caption:\", caption)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8385053d7ef698c2",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "허깅페이스 api 호출"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa58ece3d4c092ad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 Hugging Face API 토큰 가져오기\n",
    "api_token = os.getenv('HUGGING_FACE_API')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97802fa26947d5db",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: [{'generated_text': 'a close up of a cell phone with a text message'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "# 이미지 파일을 읽어서 base64로 변환\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Hugging Face API 호출 함수\n",
    "def query(image_path):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}  # Hugging Face API 토큰\n",
    "    image_data = encode_image(image_path)\n",
    "\n",
    "    # API 요청 데이터 (이미지를 base64로 변환 후 전송)\n",
    "    payload = {\n",
    "        \"inputs\": {\n",
    "            \"image\": image_data\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # API 호출\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# 이미지 파일 경로\n",
    "image_path = 'test4.png'\n",
    "\n",
    "# 결과 출력\n",
    "result = query(image_path)\n",
    "print(\"Generated caption:\", result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:43:52.122472Z",
     "start_time": "2024-11-11T16:43:49.234740Z"
    }
   },
   "id": "fd9e95ac21d06713",
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
